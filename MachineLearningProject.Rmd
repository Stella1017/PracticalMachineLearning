---
title: "Machine Learning Project"
author: "Stella Li"
output: 
  html_document: 
    keep_md: yes
---
##Introduction

###Background cited from project description
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

###Summary
1. Load original testing and training data;
2. Use a simple holdout method for cross validation: 70% of the original testing data were used to build models(myTrain data) and the rest of original testing data were used to test the models;
3. Clean data: Since there are so many variables, we removed some columns;
4. Build and test models: We first tried Decision Tree model, however the accuracy was rather low. We then tried Random Forest model and the accuracy was 0.99. Since there were still many variables, we also applied PCA to simplify model, however the accuracy of new model was not as good;
5. Apply Random Forest model (without PCA) to the original testing data (20 samples) to estimate their classes.

##Analysis
###1. Packages and Seed
We set the overall pseudo-random number generator seed as 1017. In order to reproduce the same results as below, the same seed should be used. Different packages were downloaded and installed
```{r step1}
library(caret)
library(randomForest)
library(rpart)
set.seed(1017)
```

###2. Load and split data
```{r step2}
trainURL <- c("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testURL <- c("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
training <- read.csv(url(trainURL), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testURL), na.strings=c("NA","#DIV/0!",""))
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
myTrain <- training[inTrain, ]
myTest <- training[-inTrain, ]
dim(myTrain)
```

###3. Clean data
There are 160 columns in myTrain data. We did some initial exploration to understand the dataset, and decide to remove some of the columns to clean data. To save some space, I will not print the result of summary(myTrain).
```{r step3}
## delete the columns that cannot be used for prediction
myTrain <- myTrain[ , -(1:7)]
## exclude the columns with little variance
nzv <- nearZeroVar(myTrain, saveMetrics = TRUE)
myTrain <- myTrain[ , !nzv$nzv]
## exclude the columns with doo many NAs; there are 67 columns have more than 95% NAs
NAcol <- apply(myTrain, 2, function(x) sum(is.na(x))/nrow(myTrain))
myTrain <- myTrain[ , !NAcol>0.95]
dim(myTrain)
```

###4. Prediction Models
Model 1. Decision Tree
```{r step4-decision-tree}
myfit.dt <- rpart(classe ~., data=myTrain, method="class")
## clean myTest data before testing our prediction
myTest <- myTest[ , -(1:7)]
myTest <- myTest[ , !nzv$nzv]
myTest <- myTest[ , !NAcol>0.95]
confusionMatrix(myTest$classe, predict(myfit.dt, myTest,type="class"))
```

Model 2. Random Forest
```{r step4-random-forest1}
myfit.rf1 <- randomForest(classe ~., data=myTrain)
confusionMatrix(myTest$classe, predict(myfit.rf1, myTest))
```

Model 3. Random Forest with PCA
Since there are still more than 50 variables in myTrain data, we applied PCA to removing variables that have high correlations with themselves
```{r step4-random-forest2}
## pre-process data to cover 95% of variance 
PreProc <- preProcess(myTrain[ ,-53], method="pca", thresh=0.95)
myTrainPre <- predict(PreProc, myTrain[ ,-53])
myfit.rf2 <- randomForest(myTrain$classe ~., data=myTrainPre)
myTestPre <- predict(PreProc, myTest[ ,-53])
confusionMatrix(myTest$classe, predict(myfit.rf2, myTestPre))
```

##Discussion
Random Forest models yielded better results than Decision Tree model. The accuracy for the two Random Forest models were 0.97 and 0.99 respectively. Sensitivity for all classes were between 93% to 99% and specificity were all higher than 99%.
I decided to use the Random Forest without PCA model to predict the classes of samples in the original testing dataset, as this model had a higher accuracy. However, I am aware that this model could be overfitted with so many variables.
```{r final, eval=FALSE}
predict(myfit.rf1, testing)
```
